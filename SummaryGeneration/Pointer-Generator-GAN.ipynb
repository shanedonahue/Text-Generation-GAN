{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768eee96",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Adversarial Training on Pointer Generator\n",
    "## Introduction\n",
    "    The beginning of the introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b20d6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Table Of Contents:\n",
    "* [Load Data & Initialize Model](#load-initialize)\n",
    "* [Example Generation](#example-generation)\n",
    "* [Train Pointer Generator](#train-global-1)\n",
    "    * [Without Coverage](#train-global-1-sub-1)\n",
    "        * [Generate Tokens](#gen-global-1-sub-1)\n",
    "        * [Rouge Evaluation](#rouge-global-1-sub-1)\n",
    "    * [With Coverage](#train-global-1-sub-2)\n",
    "        * [Generate Tokens](#gen-global-1-sub-2)\n",
    "        * [Rouge Evaluation](#rouge-global-1-sub-2)\n",
    "* [Train Generative Adversarial Network](#train-global-2)\n",
    "    * [Pretrain Discriminator](#train-global-2-sub-1)\n",
    "        * [Generate Tokens](#gen-global-2-sub-1)\n",
    "        * [Rouge Evaluation](#rouge-global-2-sub-1)\n",
    "    * [Adversarial Training](#train-global-2-sub-2)\n",
    "        * [Generate Tokens](#gen-global-2-sub-2)\n",
    "        * [Rouge Evaluation](#rouge-global-2-sub-2)\n",
    "* [Analysis & Conclusion](#analysis-conclusion)\n",
    "* [Limitations & Future Work](#limit-future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b433a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data & Initialize Model <a class=\"anchor\" id=\"load-initialize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data import Data\n",
    "from model import SummaryModel\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.logging.set_verbosity('ERROR')\n",
    "\n",
    "parser = argparse.ArgumentParser(description = 'Train/Test summarization model', formatter_class = argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Import Setting\n",
    "parser.add_argument(\"--doc_file\", type = str, default = './data/doc.p', help = 'path to document file')\n",
    "parser.add_argument(\"--vocab_file\", type = str, default = './data/vocab.p', help = 'path to vocabulary file')\n",
    "parser.add_argument(\"--emb_file\", type = str, default = './data/emb.p', help = 'path to embedding file')\n",
    "parser.add_argument(\"--src_time\", type = int, default = 200, help = 'maximal # of time steps in source text')\n",
    "parser.add_argument(\"--sum_time\", type = int, default = 50, help = 'maximal # of time steps in summary')\n",
    "parser.add_argument(\"--max_oov_bucket\", type = int, default = 280, help = 'maximal # of out-of-vocabulary word in one summary')\n",
    "parser.add_argument(\"--train_ratio\", type = float, default = 0.8, help = 'ratio of training data')\n",
    "parser.add_argument(\"--seed\", type = int, default = 888, help = 'seed for spliting data')\n",
    "\n",
    "# Saving Setting\n",
    "parser.add_argument(\"--log\", type = str, default = './log/', help = 'logging directory')\n",
    "parser.add_argument(\"--save\", type = str, default = './model/', help = 'model saving directory')\n",
    "parser.add_argument(\"--checkpoint\", type = str, help = 'path to checkpoint point')\n",
    "parser.add_argument(\"--autosearch\", type = bool, default = False, help = \"[NOT AVAILABLE] Set 'True' if searching for latest checkpoint\")\n",
    "parser.add_argument(\"--save_interval\", type = int, default = 1900, help = \"Save interval for training\")\n",
    "\n",
    "# Hyperparameter Setting\n",
    "parser.add_argument(\"--batch_size\", type = int, default = 16, help = 'number of samples in one batch')\n",
    "parser.add_argument(\"--gen_lr\", type = float, default = 1e-3, help = 'learning rate for generator')\n",
    "parser.add_argument(\"--dis_lr\", type = float, default = 1e-3, help = 'learning rate for discriminator')\n",
    "parser.add_argument(\"--cov_weight\", type = float, default = 1e-3, help = 'learning rate for coverage')\n",
    "\n",
    "params = vars(parser.parse_args([]))\n",
    "\n",
    "params['load_pretrain'] = True\n",
    "# 1900 no coverage\n",
    "# \n",
    "params['checkpoint'] = './model/pointer_cov_supervised-1900' # Uncomment when requiring reloading model\n",
    "\n",
    "model = SummaryModel(**params)\n",
    "data = Data(**params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Generation <a class=\"anchor\" id=\"example-generation\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = data.get_next_epoch()\n",
    "test_data = data.get_next_epoch_test()\n",
    "src, ref, gen, tokens, scores, attens, gt_attens = None, None, None, None, None, None, None\n",
    "for feed_dict in train_data:\n",
    "    real, fake, real_len, fake_len = model.sess.run(\n",
    "        [model.real_reward, model.fake_reward, model.sum_len, model.tokens_len], feed_dict=feed_dict)\n",
    "    print(np.mean(real[1, 0:int(real_len[1])]))\n",
    "    print(np.mean(fake[1, 0:int(fake_len[1])]))\n",
    "    break\n",
    "\n",
    "for feed_dict in test_data:\n",
    "    tokens, scores, attens = model.beam_search(feed_dict)\n",
    "    src, ref, gen = data.id2word(feed_dict, tokens)\n",
    "    gt_attens = model.sess.run(model.atten_dist, feed_dict = feed_dict)\n",
    "#     print(src, ref, gen, gt_attens)\n",
    "    x = 0\n",
    "    print (\"\".join(src[x]).replace(\"(OOV)\",\"\"), end = '\\n\\n')\n",
    "    print (\"\".join(ref[x]).replace(\"(OOV)\",\"\"), end = '\\n\\n')\n",
    "    print (gen)\n",
    "#     for i in range(len(src)):\n",
    "#         print (\"\".join(gen[x][i]))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Pointer Generator<a class=\"anchor\" id=\"train-global-1\"></a>\n",
    "### Train without coverage<a class=\"anchor\" id=\"train-global-1-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_max_epoch = 1\n",
    "print (f'Start from step {model.sess.run(model.gen_global_step)}')\n",
    "for i in range(train_max_epoch):\n",
    "    print (f'Train Epoch {i}')\n",
    "    train_data = data.get_next_epoch()\n",
    "    model.train_one_epoch(train_data, data.n_train_batch, coverage_on = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate tokens <a class=\"anchor\" id=\"gen-global-1-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_top_k_tokens(coverage, top_k=1):\n",
    "    test_data = data.get_next_epoch_test()\n",
    "    src = [[] for i in range(top_k)]\n",
    "    ref = [[] for i in range(top_k)]\n",
    "    gen = [[] for i in range(top_k)]\n",
    "    for feed_dict in test_data:\n",
    "        tokens, scores, attens = model.beam_search(feed_dict, coverage_on = coverage, top_k = top_k)\n",
    "        if top_k == 1:\n",
    "            tokens = [tokens]\n",
    "            scores = [scores]\n",
    "        for i in range(top_k):\n",
    "            src[i], ref[i], gen[i] = data.id2word(feed_dict, tokens[i])\n",
    "#         feed_dict['coverage_on:0'] = coverage\n",
    "#         gt_attens = model.sess.run(model.atten_dist, feed_dict = feed_dict)\n",
    "        break\n",
    "    return src, ref, gen, scores\n",
    "\n",
    "def clean_generated_tokens(src, ref, gen):\n",
    "    src_str_list = [\"\".join(src[0][i]).replace(\"(OOV)\", \"\") for i in range(len(src[0]))]\n",
    "    ref_str_list = [\"\".join(ref[0][0]).replace(\"(OOV)\", \"\") for i in range(len(ref[0]))]\n",
    "    gen_str_list = []\n",
    "    for i in range(len(test1_gen)):\n",
    "        gen_str_list.append([\"\".join(gen[i][j]).replace(\"(OOV)\",\"\") for j in range(len(gen[i]))])\n",
    "    return src_str_list, ref_str_list, gen_str_list\n",
    "\n",
    "def print_cleaned_tokens(src, ref, gen, scores):\n",
    "    for i in range(len(gen)):\n",
    "        print (\"Batch: \" + str(i))\n",
    "        for j in range(len(gen[i])):\n",
    "            print(\"\\tGeneration: \" + str(j))\n",
    "            print (\"\\t\\tAbstract: \" + src[j][:min(95, len(src[j]))])\n",
    "            print (\"\\t\\tTitle: \"+ ref[j])\n",
    "            print(\"\\t\\tGenerated: \" + gen[i][j])\n",
    "            print(\"\\t\\tScore: \" + str(scores[i][j]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1_src, test1_ref, test1_gen, test1_scores = generate_top_k_tokens(coverage=False, top_k=1)\n",
    "test1_src_list, test1_ref_list, test1_gen_list = clean_generated_tokens(test1_src, test1_ref, test1_gen)\n",
    "print_cleaned_tokens(test1_src_list, test1_ref_list, test1_gen_list, test1_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rouge Evaluation<a class=\"anchor\" id=\"rouge-global-1-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from tqdm import tqdm_notebook\n",
    "rouge = Rouge()\n",
    "\n",
    "def generate_evaluation_sets(coverage):\n",
    "    refs = []\n",
    "    gens = []\n",
    "    cnt = 0\n",
    "    test_n = min(data.n_test_batch, 50)\n",
    "    test_data = data.get_next_epoch_test()\n",
    "    for feed_dict in tqdm_notebook(test_data, total = test_n):\n",
    "        tokens, scores, attens = model.beam_search(feed_dict, coverage_on = coverage)\n",
    "        # sample_tokens = model.sess.run(model.tokens, feed_dict = feed_dict)\n",
    "        src, ref, gen = data.id2word(feed_dict, tokens)\n",
    "        for i in range(len(ref)):\n",
    "            refs.append(\" \".join(ref[i][:-1]))\n",
    "            gens.append(\" \".join(gen[i][:-1]))\n",
    "        cnt += 1\n",
    "        if cnt > test_n:\n",
    "            break\n",
    "    new_gens = []\n",
    "    new_refs = []\n",
    "    for i in range(len(gens)):\n",
    "        if not (gens[i] == \"\"):\n",
    "            new_gens.append(gens[i])\n",
    "            new_refs.append(refs[i])\n",
    "    return new_refs, new_gens\n",
    "\n",
    "\n",
    "def rouge_evaluation(ref, gen):\n",
    "    rouge_score = rouge.get_scores(gen, ref)\n",
    "    r1, r2, rl = 0., 0., 0.\n",
    "    for score in rouge_score:\n",
    "        r1 = r1 + score['rouge-1']['f']\n",
    "        r2 = r2 + score['rouge-2']['f']\n",
    "        rl = rl + score['rouge-l']['f']\n",
    "    r1 /= len(rouge_score)\n",
    "    r2 /= len(rouge_score)\n",
    "    rl /= len(rouge_score)\n",
    "    print (r1, r2, rl)\n",
    "    return r1, r2, rl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1_eval_refs, test1_eval_gens = generate_evaluation_sets(coverage=False)\n",
    "test1_r1, test1_r2, test1_rl = rouge_evaluation(test1_eval_refs, test1_eval_gens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train with coverage<a class=\"anchor\" id=\"train-global-1-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_max_epoch = 1\n",
    "print (f'Start from step {model.sess.run(model.gen_global_step)}')\n",
    "for i in range(train_max_epoch):\n",
    "    print (f'Train Epoch {i}')\n",
    "    train_data = data.get_next_epoch()\n",
    "    model.train_one_epoch(train_data, data.n_train_batch, coverage_on = True, model_name = 'with_coverage')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate tokens<a class=\"anchor\" id=\"gen-global-1-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test2_src, test2_ref, test2_gen, test2_scores = generate_top_k_tokens(coverage=True, top_k=1)\n",
    "test2_src_list, test2_ref_list, test2_gen_list = clean_generated_tokens(test2_src, test2_ref, test2_gen)\n",
    "print_cleaned_tokens(test2_src_list, test2_ref_list, test2_gen_list, test2_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rouge Evaluation<a class=\"anchor\" id=\"rouge-global-1-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test2_eval_refs, test2_eval_gens = generate_evaluation_sets(coverage=True)\n",
    "test2_r1, test2_r2, test2_rl = rouge_evaluation(test2_eval_refs, test2_eval_gens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train GAN<a class=\"anchor\" id=\"train-global-2\"></a>\n",
    "### Pretrain Discriminator<a class=\"anchor\" id=\"train-global-2-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_max_epoch = 1\n",
    "print (f'Start from step {model.sess.run(model.gen_global_step_2)}')\n",
    "for i in range(train_max_epoch):\n",
    "    print (f'Train Epoch {i}')\n",
    "    train_data = data.get_next_epoch()\n",
    "    model.train_one_epoch_pre_dis(train_data, data.n_train_batch, coverage_on = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate tokens<a class=\"anchor\" id=\"gen-global-2-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test3_src, test3_ref, test3_gen, test3_scores = generate_top_k_tokens(coverage=True, top_k=1)\n",
    "test3_src_list, test3_ref_list, test3_gen_list = clean_generated_tokens(test3_src, test3_ref, test3_gen)\n",
    "print_cleaned_tokens(test3_src_list, test3_ref_list, test3_gen_list, test3_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rouge Evaluation<a class=\"anchor\" id=\"rouge-global-2-sub-1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test3_eval_refs, test3_eval_gens = generate_evaluation_sets(coverage=True)\n",
    "test3_r1, test3_r2, test3_rl = rouge_evaluation(test3_eval_refs, test3_eval_gens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adversarial Training<a class=\"anchor\" id=\"train-global-2-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_max_epoch = 12\n",
    "print (f'Start from step {model.sess.run(model.gen_global_step_2)}')\n",
    "for i in range(train_max_epoch):\n",
    "    print (f'Train Epoch {i}')\n",
    "    train_data = data.get_next_epoch()\n",
    "    model.train_one_epoch_unsup(train_data, data.n_train_batch, coverage_on = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate Tokens<a class=\"anchor\" id=\"gen-global-2-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test4_src, test4_ref, test4_gen, test4_scores = generate_top_k_tokens(coverage=True, top_k=1)\n",
    "test4_src_list, test4_ref_list, test4_gen_list = clean_generated_tokens(test4_src, test4_ref, test4_gen)\n",
    "print_cleaned_tokens(test4_src_list, test4_ref_list, test4_gen_list, test4_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rouge Evaluation<a class=\"anchor\" id=\"rouge-global-2-sub-2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test4_eval_refs, test4_eval_gens = generate_evaluation_sets(coverage=True)\n",
    "test4_r1, test4_r2, test4_rl = rouge_evaluation(test4_eval_refs, test4_eval_gens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = data.get_next_epoch_test()\n",
    "src, ref, gen, tokens, scores, attens, gt_attens = None, None, None, None, None, None, None\n",
    "for feed_dict in train_data:\n",
    "    real, fake, real_len, fake_len = model.sess.run(\n",
    "        [model.real_reward, model.fake_reward, model.sum_len, model.tokens_len], feed_dict=feed_dict)\n",
    "    print(np.mean(real[1, 0:int(real_len[1])]))\n",
    "    print(np.mean(fake[1, 0:int(fake_len[1])]))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d4408dae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analysis & Conclusion<a class=\"anchor\" id=\"analysis-conclusion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88efb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e18d5923",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Limitations & Future Work<a class=\"anchor\" id=\"limit-future\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ccb98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}